# Python modules

import copy
import collections

# 3rd party modules
import numpy as np
import xml.etree.cElementTree as ElementTree

# Our modules
import vespa.analysis.constants as constants
import vespa.analysis.block_raw as block_raw
import vespa.analysis.block_raw_probep as block_raw_probep
import vespa.analysis.block_raw_cmrr_slaser as block_raw_cmrr_slaser
import vespa.analysis.block_raw_edit_fidsum as block_raw_edit_fidsum
import vespa.analysis.block_prep_identity as block_prep_identity
import vespa.analysis.block_prep_fidsum as block_prep_fidsum
import vespa.analysis.block_prep_timeseries as block_prep_timeseries
import vespa.analysis.block_prep_wbnaa as block_prep_wbnaa
import vespa.analysis.block_prep_edit_fidsum as block_prep_edit_fidsum
import vespa.analysis.block_spectral as block_spectral
import vespa.analysis.block_spectral_identity as block_spectral_identity
import vespa.analysis.block_fit_voigt as block_fit_voigt
import vespa.analysis.block_fit_giso as block_fit_giso
import vespa.analysis.block_fit_identity as block_fit_identity
import vespa.analysis.block_quant_watref as block_quant_watref
import vespa.analysis.block_quant_identity as block_quant_identity
import vespa.analysis.mrs_user_prior as mrs_user_prior

import vespa.common.util.misc as util_misc
import vespa.common.util.xml_ as util_xml
import vespa.common.constants as common_constants
import vespa.common.mrs_data_raw as mrs_data_raw
import vespa.common.mrs_data_raw_timeseries as mrs_data_raw_timeseries

from vespa.common.constants import Deflate
from vespa.analysis.constants import FitMacromoleculeMethod
from vespa.analysis.block_prep_fidsum import BlockPrepFidsum
from vespa.common.mrs_data_raw import DataRawFidsum



# The HTML generated by this code contains a conditional comment for IE users.
# _IE_INCAPABLE_MSG is the content for that comment.
# We use the data URI scheme to display our image, and IE < 9 does support it.
# The conditionalcomment is a valid HTML comment. It's ignored by browsers
# except IE which is documented to look for HTML comments containing certain key
# strings. This allows us to display slightly different content to those who
# can't see the image.
# ref: http://msdn.microsoft.com/en-us/library/ms537512%28v=vs.85%29.aspx
# reF: http://www.quirksmode.org/css/condcom.html
_IE_INCAPABLE_MSG = """[if lt IE 9]>
<p><strong>Sorry, Internet Explorer 6, 7, and 8 can't display the 
image below. All other major browsers (IE 9, 
<a href="http://www.mozilla.org/">Firefox</a>, 
<a href="http://www.google.com/chrome/">Chrome</a>,
<a href="http://www.opera.com/">Opera</a>, and
<a href="http://www.apple.com/safari/">Safari</a>) 
can display it.</strong>
</p>
<![endif]
"""


# _CSS is the style sheet for the HTML that this code builds. Note that 
# wxPython's HTML control doesn't understand CSS. This is only for printing
# from the browser.
_CSS = """
    @page { margin: 20mm;          
            size: landscape;  
          }

    @media print {
        /* a4 = 210 x 297mm, US letter = 216 x 280mm. Two 20mm margins plus
        two 110mm block elements = 40 + 220 = 260mm which fits with room to
        spare on both paper sizes as long as the 'landscape' directive is
        respected.
        */
        div#table { width: 80mm; }
        div#img   { width: 140mm; }
    }
"""


# The 3 functions below are used for building HTML
def _format_column(column, places=4):
    # Given a column from the table, converts it to a nicely formatted string
    # and returns it. The column can be float, int, bool or a string. Strings
    # are returned untouched.
    if isinstance(column, float):
        column = ("%%.%dg" % places) % column
    elif isinstance(column, int) or isinstance(column, bool):
        column = str(column)
    #else:
        # It's a string, leave it alone.

    return column


def _get_max_width(table, index):
    """Get the maximum width of the given column index"""
    return max([len(_format_column(row[index])) for row in table])    


def _pretty_space_table(table, places, pad=2):
    """Returns a table of data, padded for alignment
    @param table: The table to print. A list of lists.
    Each row must have the same number of columns. 
    """
    col_paddings = []

    for i in range(len(table[0])):
        col_paddings.append(_get_max_width(table, i))

    lines = []
    for row in table:
        # left col
        hdr = row[0].center(col_paddings[0] + pad)
        # rest of the cols
        for i in range(1, len(row)):
            col = _format_column(row[i], places).center(col_paddings[i] + pad)
            hdr += col
        lines.append(hdr)
    
    return lines


def _pretty_space_table_inplace(table, places, pad=2):
    """Returns a table of data, padded for alignment
    @param table: The table to print. A list of lists.
    Each row must have the same number of columns. 
    """
    col_paddings = []

    for i in range(len(table[0])):
        col_paddings.append(_get_max_width(table, i))

    for j, row in enumerate(table):
        # left col
        table[j][0] = row[0].rjust(col_paddings[0] + pad)
        # rest of the cols
        for i in range(1, len(row)):
            table[j][i] = _format_column(row[i], places).center(col_paddings[i] + pad)
    
    return table




###########    Start of "private" constants, functions and classes    #########

_HLSVD_FILTER_UUID = "14023031-ad45-4662-a89c-4e35d8732244"     # deprecated, so hard wired

# _XML_TAG_TO_SLOT_CLASS_MAP maps XML element tags to 2-tuples of
# (slot name, block class) where slot name is one of "raw", "prep", "spectral",
# or "fit" and the class can be any class appropriate for that slot.
_XML_TAG_TO_SLOT_CLASS_MAP = {
    "block_raw"                 : ("raw", block_raw.BlockRaw),
    "block_raw_probep"          : ("raw", block_raw_probep.BlockRawProbep),
    "block_raw_cmrr_slaser"     : ("raw", block_raw_cmrr_slaser.BlockRawCmrrSlaser),
    "block_raw_edit_fidsum"     : ("raw", block_raw_edit_fidsum.BlockRawEditFidsum),
    "block_prep_identity"       : ("prep", block_prep_identity.BlockPrepIdentity),
    "block_prep_fidsum"         : ("prep", block_prep_fidsum.BlockPrepFidsum),
    "block_prep_timeseries"     : ("prep", block_prep_timeseries.BlockPrepTimeseries),
    "block_prep_edit_fidsum"    : ("prep", block_prep_edit_fidsum.BlockPrepEditFidsum),
    "block_prep_wbnaa"          : ("prep", block_prep_wbnaa.BlockPrepWbnaa),
    "block_spectral_identity"   : ("spectral", block_spectral_identity.BlockSpectralIdentity),
    "block_spectral"            : ("spectral", block_spectral.BlockSpectral),
    "block_fit_identity"        : ("fit", block_fit_identity.BlockFitIdentity),
    # block_voigt is an older name for what is now block_fit_voigt.
    "block_voigt"               : ("fit", block_fit_voigt.BlockFitVoigt),
    "block_fit_voigt"           : ("fit", block_fit_voigt.BlockFitVoigt),
    "block_fit_giso"            : ("fit", block_fit_giso.BlockFitGiso),
    "block_quant_watref"        : ("quant", block_quant_watref.BlockQuantWatref),
}

# DEFAULT_BLOCK_CLASSES defines which block slots are filled and with which
# classes when a Dataset object is instantiated. We use mostly identity
# classes which means they're not very useful as-is but they serve as
# lightweight placeholders.
# This dict is also a model for custom dicts that might get passed to
# dataset_from_raw().
# This dict should be read-only, and if Python had a read-only dict I would
# use it here. Don't modify it at runtime.
DEFAULT_BLOCK_CLASSES = {
    "raw"       : block_raw.BlockRaw,
    "prep"      : block_prep_identity.BlockPrepIdentity,
    "spectral"  : block_spectral_identity.BlockSpectralIdentity,
    "fit"       : block_fit_identity.BlockFitIdentity,
    "quant"     : block_quant_identity.BlockQuantIdentity,
}


###########    End of "private" constants, functions and classes    #########



############    Start of public constants, functions and classes    ##########


def dataset_from_raw(raw, block_classes={ }, zero_fill_multiplier=0):
    """
    Given a DataRaw object and some optional params, returns a dataset
    containing the minimal set of operational blocks for use in Analysis: raw
    and spectral. Other block slots (prep and fit) contain identity blocks.

    If block_classes is populated, it must be a dict that maps slot names
    to block classes like the dict DEFAULT_BLOCK_CLASSES in this module. The
    classes specified will be used in the slots specified.

    It's not necessary to specify all slots. For instance, to force data to
    be banana-shaped, a caller could pass a dict like this:
        { "prep" : block_prep_banana.BlockPrepBanana }

    If the zero_fill_multiplier is non-zero, it is applied to the newly-created
    dataset.
    """
    dataset = Dataset()

    # Replace the default raw block. Note that even if the default raw block is of the
    # correct type (which is probably the case), I can't just call its inflate() method
    # because _create_block() does other stuff besides just inflating the block.
    klass = block_classes.get("raw", block_raw.BlockRaw)
    dataset._create_block(("raw", klass), raw.deflate(Deflate.DICTIONARY))

    if "prep" in block_classes:
        # Replace the default prep block
        dataset._create_block( ("prep", block_classes["prep"]) )
    #else:
        # The default identity block is just fine.

    klass = block_classes.get("spectral", block_spectral.BlockSpectral)
    dataset._create_block(("spectral", klass))

    # At present, we never create/replace a fit block via this method.

    # At present, we never create/replace a quant block via this method.

    # Update prior spectra now that the spectral block exists.
    dataset.user_prior.basis.update(dataset)
    #if isinstance(dataset.blocks['prep'], DataRawFidsum):
    if isinstance(dataset.blocks['prep'], BlockPrepFidsum):
        dataset.blocks['prep'].set.vespa_preprocess_prior.basis.update(dataset, zfmult=4)

    # Adjust the zerofill if necessary.
    if zero_fill_multiplier:
        dataset.update_for_zerofill_change(zero_fill_multiplier)

    return dataset


class Dataset(object):
    """
    This is the primary data object for the Analysis program.

    On the GUI side, the Analysis application is just a notebook filled with
    dataset Tabs. These can be opened, closed, and organized on screen in many
    ways, but each dataset Tab contains only one Dataset object.

    A Dataset object has a 'blocks' attribute which is an ordered dictionary of
    Block objects. Taken together, the 'blocks' comprise the entire processing
    workflow for the MRS data. The 'blocks' dict is keyed with the names "raw",
    "prep", "spectral", "fit", and "quant" in that order. These represent
    "slots", each of which can be filled with one block of the appropriate type.

    The first slot, 'raw', is a Block containg the raw Data. It does no
    processing, but just holds the incoming data and information about the data
    and its header. All the other blocks have the option of transforming the
    data somehow. When a slot's transformation doesn't need to do anything, it
    is filled with a lightweight version of the block that just performs the
    identity transform on the data.

    Each Block has a 'Settings' object that contain the processing parameter
    settings that describe how to process the data in a Block. These are kept
    in a separate object within each block to delineate the inputs from the
    outputs variables and makes it easier to create 'Preset' files for batch
    processing from an existing Dataset.

    Each Block also has a 'data' attribute that contains the state of the data
    after processing. There may also be additional results attributes that hold
    intermediate processing steps results. Each Block references the previous
    step to get the input data for the current processing step.

    Each Block has a 'Chain' object that performs the actual scientific
    algorithms on the data. Chain objects are created at run-time and are not
    saved. The Block, Tab, and Chain objects are how Analysis implements the
    Model-View-Controller paradigm. Only the Block objects are saved (when the
    user chooses File->Save). All other objects are recreated at run-time.

    """
    # The XML_VERSION enables us to change the XML output format in the future
    XML_VERSION = "1.2.0"

    def __init__(self, attributes=None):
        #------------------------------------------------------------
        # Set up elements of the processing chain for the data set
        #
        # Each dataset tab is located in the "outer" notebook which
        # is used to organize the one or more data sets that are open
        # in the application.
        #
        # - The blocks list contain objects that correspond to the
        #   processing tabs in the dataset (or "inner") notebook. A
        #   "block" contains a "chain" object that contains
        #   the code to run the functor chain of processing steps for
        #   a given block. The functor chain is dynamically allocated
        #   for each call depending on the widget settings in the tab.
        #
        #------------------------------------------------------------

        self.id = util_misc.uuid()

        # dataset_filename is only set on-the-fly (as opposed to being set
        # via inflate()). It's only set when the current dataset is read in,
        # or saved to, a VIFF file.
        self.dataset_filename = ''

        self.behave_as_preset = False

        # preset_filename only set if apply_preset() is called, it is just for 
        # provenance, we do NOT keep track if setting changed manually afterwards
        self.preset_filename  = ''

        self.user_prior = mrs_user_prior.UserPrior()

        # Create default blocks. We replace these as needed. Note that the
        # order in which these are added drives the order of the blocks and
        # tabs in the application as a whole.
        self.blocks = collections.OrderedDict()
        for name in ("raw", "prep", "spectral", "fit", "quant"):
            self._create_block( (name, DEFAULT_BLOCK_CLASSES[name]) )

        if attributes is not None:
            self.inflate(attributes)

        # Update the user prior spectrum and fid caches
        self.user_prior.basis.update(self)


    @property
    def raw_shape(self):
        """Raw data dimensionality. It's read only."""
        if self.blocks:
            return self.blocks["raw"].data_shape
        return None

    @property
    def raw_dims(self):
        """Raw data dimensionality. It's read only."""
        if self.blocks:
            if self.blocks["prep"].is_identity:
                return self.blocks["raw"].dims
            else:
                return self.blocks["prep"].dims
        return None

    @property
    def spectral_dims(self):
        """Spectral data dimensionality. It's read only."""
        if self.blocks:
            spectral_dims = self.raw_dims
            zfmult   = self.zero_fill_multiplier
            if zfmult:
                spectral_dims[0] *= zfmult
                return spectral_dims
        return None

    @property
    def spectral_hpp(self):
        """Raw/All data center frequency. It's read only."""
        if self.blocks:
            spectral_dims = self.spectral_dims
            if spectral_dims:
                return self.sw / spectral_dims[0]
        return None

    @property
    def sw(self):
        """Raw/All data sweep width. It's read only."""
        return self.blocks["raw"].sw if self.blocks else None

    @property
    def raw_hpp(self):
        """Raw/All data center frequency. It's read only."""
        return self.sw / self.raw_dims[0] if self.blocks else None

    @property
    def frequency(self):
        """Raw/All data center frequency. It's read only."""
        return self.blocks["raw"].frequency if self.blocks else None

    @property
    def resppm(self):
        """Raw/All data resonance PPM value. It's read only."""
        return self.blocks["raw"].resppm if self.blocks else None

    @property
    def echopeak(self):
        """ Acquisition echo peak (0.0 for FID data, 0.5 for full echo) """
        return self.blocks["raw"].echopeak if self.blocks else None

    @property
    def is_fid(self):
        """Boolean. It's read only."""
        return self.blocks["raw"].is_fid if self.blocks else None

    @property
    def seqte(self):
        """Acquisition echo time in msec. It's read only."""
        return self.blocks["raw"].seqte if self.blocks else None

    @property
    def seqtr(self):
        """Acquisition repetition time in msec. It's read only."""
        return self.blocks["raw"].seqtr if self.blocks else None

    @property
    def nucleus(self):
        """Acquisition nucleus. It's read only."""
        return self.blocks["raw"].nucleus if self.blocks else None

    @property
    def zero_fill_multiplier(self):
        """Spectral dimension zero fill factor. It's read only."""
        return self.blocks["spectral"].set.zero_fill_multiplier if self.blocks else None

    @property
    def phase_1_pivot(self):
        """Spectral phase 1 pivot location in ppm. It's read only."""
        return self.blocks["spectral"].set.phase_1_pivot if self.blocks else None

    @property
    def auto_b0_range_start(self):
        """ PPM start range for automated B0 shift routine searches """
        return self.user_prior.auto_b0_range_start

    @property
    def auto_b0_range_end(self):
        """ PPM end range for automated B0 shift routine searches """
        return self.user_prior.auto_b0_range_end

    @property
    def auto_phase0_range_start(self):
        """ PPM start range for automated Phase0 shift routine searches """
        return self.user_prior.auto_phase0_range_start

    @property
    def auto_phase0_range_end(self):
        """ PPM end range for automated Phase0 shift routine searches """
        return self.user_prior.auto_phase0_range_end

    @property
    def auto_phase1_range_start(self):
        """ PPM start range for automated Phase1 shift routine searches """
        return self.user_prior.auto_phase1_range_start

    @property
    def auto_phase1_range_end(self):
        """ PPM end range for automated Phase1 shift routine searches """
        return self.user_prior.auto_phase1_range_end

    @property
    def auto_phase1_pivot(self):
        """ PPM value at which automated Phase1 routine rotates phase """
        return self.user_prior.auto_phase1_pivot

    @property
    def metinfo(self):
        """
        Returns the Metinfo object stored in Dataset. This provides info about
        literature values of metabolites, such as concentrations and spins that
        are used in the fitting initial values routines.
        """
        return self.user_prior.metinfo

    @property
    def user_prior_summed_spectrum(self):
        """
        Returns Numpy array with frequency spectrum created from the UserPrior
        values in that dialog. This is the model spectrum used in the automated
        B0 and Phase routines. This spectrum matches the spectral resolution of
        the data. Obtained from UserPrior object in Dataset. Read only!
        """
        return self.user_prior.basis.get_spectrum_sum(self)

    @property
    def all_voxels(self):
        """ return list of all voxel indices based on spectral_dims """
        dims = self.spectral_dims
        all = []
        for k in range(dims[3]):
            for j in range(dims[2]):
                for i in range(dims[1]):
                    all.append((i,j,k))
        return all

    @property
    def measure_time(self):
        """
        Returns a Numpy array of floating point values. These values are the 
        acquisition times for each FID in the data array. Each value is the 
        time since midnight in seconds at which the acquisition started. Most
        use cases will normalize this value into 0 for the first FID and some
        delta seconds from 0 for each subsequent FID.
        
        This functions gathers measure_time values from the Prep block since
        the number of FIDs may change if we 'massage' the data. Eg. if we 
        average every other or every 3/4/N FIDs to improve SNR. In this case
        the new measure_time array is calculted in the Prep block/functor. If
        no measure_time attribute is available in Prep block, we look for one
        in the Raw block. If there is none in Raw we default to a 'range' of
        integer steps.
        
        Eg. For data taken from DICOM files, the 'measure_time' tag is used.
        This tag stores the time since midnight in an HHMMSS.fraction string. 
        We convert this string to seconds in float format. 
        
        Eg. At this time we don't have other examples, but anyone writing code
        for this attribute could just start from 0 and increment as desired. 
        Any subsequent normalization would subtract from the first point and
        it would work just fine.
        
        """
        if self.blocks:
            if self.blocks["prep"].is_identity:
                block = self.blocks["raw"]
                try:
                    val = block.measure_time
                except AttributeError:
                    val = list(range(raw_dims[1]))
            else:
                block = self.blocks["prep"]
                try:
                    val = block.measure_time
                except AttributeError:
                    block = self.blocks["raw"]
                    try:
                        val = block.measure_time
                    except AttributeError:
                        val = list(range(raw_dims[1]))
        return val


    @property
    def prior_list_unique(self):
        """ 
        Get list of metabolites in the prior set listed only as unique abbreviations.
        This makes it easier for me to check if a fitting condition can be set. 
        
        """
        metinfo = self.user_prior.metinfo
        prior_list = self.blocks['fit'].set.prior_list
        prior_list_unique = [metinfo.get_abbreviation(item.lower()) for item in prior_list]
        return prior_list_unique


    @property
    def minppm(self):
        return self.pts2ppm(self.spectral_dims[0])

    @property
    def maxppm(self):
        return self.pts2ppm(0)

    @property
    def minmaxppm(self):
        return self.minppm, self.maxppm


    def ppm2pts(self, val, acq=False, rel=False):
        """
        Returns the point index along spectrum for given ppm value.
        - Assumes center point <--> resppm for rel False
        - Assumes center point <--> 0.0 ppm for rel True

        """
        dim0 = self.raw_dims[0] if acq else self.spectral_dims[0]
        hpp = self.raw_hpp if acq else self.spectral_hpp
        pts = self.frequency*val/hpp if rel else (dim0/2) - (self.frequency*(val-self.resppm)/hpp)
        pts = np.where(pts > 0, pts, 0)
        return pts

    def ppm2hz(self, val, acq=False, rel=False):
        """
        Returns the absolute number of hz away from 0.0 ppm based on an assumed ppm
        value for the center data point.

        If rel=True, assumes that center point is 0.0 ppm and calculates the
        relative hertz away represented by the ppm value.

        """
        hpp = self.raw_hpp if acq else self.spectral_hpp
        ppm = self.pts2hz(self.ppm2pts(val)) if rel else self.ppm2pts(val, rel=rel) * hpp
        return ppm

    def pts2ppm(self, val, acq=False, rel=False):
        """
        Returns the ppm value of the given point index along spectrum.
        - Assumes center point <--> resppm for rel False
        - Assumes center point <--> 0.0 ppm for rel True

        """
        dim0 = self.raw_dims[0] if acq else self.spectral_dims[0]
        hpp = self.raw_hpp if acq else self.spectral_hpp
        ppm = val*hpp/self.frequency if rel else (((dim0/2)-val)*(hpp/self.frequency))+self.resppm
        return ppm

    def pts2hz(self, val, acq=False, rel=False):
        """
        Returns the number of hertz away from 0.0 ppm from the points based on an
        assumed ppm value for the center point.

        If rel=True, assumes that center point is 0.0 ppm and calculates the
        relative hz away represented by the points value.

        """
        hpp = self.raw_hpp if acq else self.spectral_hpp
        hz = val * hpp if rel else (self.ppm2pts(0.0) - val) * hpp
        return hz

    def hz2ppm(self, val, acq=False, rel=False):
        """
        Returns the number of ppm from hertz based on an assumed ppm value for the
        center point.

        If rel=True, it is assumed that the hertz value is relative to 0.0 ppm
        equals 0.0 hertz. Thus we convert the hz value to points, take the distance
        in points from the 0.0 ppm point and convert that to ppm

        """
        hpp = self.raw_hpp if acq else self.spectral_hpp
        val = self.pts2ppm(self.hz2pts(val)) if rel else self.pts2ppm(val / hpp)
        return val

    def hz2pts(self, val, acq=False, rel=False):
        """
        Returns the number of points away from 0.0 hertz (0.0 ppm) based on an
        assumed ppm value for the center point.

        If rel=True, it is assumed that the hertz value is relative to 0.0 ppm
        equals 0.0 hertz. Thus we convert the hz value to points, take the distance
        in points from the 0.0 ppm point and convert that to points.

        """
        hpp = self.raw_hpp if acq else self.spectral_hpp
        pts = val / hpp if rel else self.ppm2pts(0.0) - (val / hpp)
        return pts


    def __str__(self):

        lines = []
        lines.append("------- {0} Object -------".format(self.__class__.__name__))
        lines.append("filename: %s" % self.dataset_filename)
        for block in self.blocks.values():
            lines.append(str(block))

        return '\n'.join(lines)


##########################    Public methods    #########################

    def add_fit(self, type, force=False, full_range=False):
        """
        Replaces the fit identity block with a typed fitting block. If the
        current fit block is not an indentiy block, this code does nothing.

        The force flag is used in the apply_preset method to force a new
        'fit' object to be created if needed

        Returns the new fit block if one was created, or None otherwise.

        """
        if self.blocks["fit"].is_identity or force:
            # The existing Fit block is the Identity block. Replace it.
            # FIXME bjs Adding a fit block does a calc_full_basis_set()
            block = self._create_block(type)

            ppm_str = block.set.prior_ppm_start
            ppm_end = block.set.prior_ppm_end
            if full_range:
                ppm_str = None
                ppm_end = None

            prior = block.set.prior
            prior.calculate_full_basis_set(ppm_str, ppm_end, self)
            return block
        else:
            # The existing Fit block is not the Identity block. Don't touch it.
            return None


    def add_voigt(self, force=False):
        """ replace fit identity block with a FitVoigt block """
        return self.add_fit('block_fit_voigt')


    def add_giso(self, force=False):
        """ replace fit identity block with a FitGiso block """
        return self.add_fit('block_fit_giso')


    def add_watref(self, force=False):
        """
        Replaces the quant identity block with a QuantWatref block. If the
        current fit block is not an indentiy block, this code does nothing.

        The force flag is used in the apply_preset method to force a new
        'fit' object to be created if needed

        Returns the new fit voigt block if one was created, or None otherwise.
        """
        if self.blocks["quant"].is_identity or force:
            # The existing Quant block is the Identity block. Replace it.
            block = self._create_block("block_quant_watref")

            return block
        else:
            # The existing Quant block is not the Identity block. Don't touch it.
            return None


    def batch_process_all(self, statusbar=None):

        # TODO - bjs
        #
        # Error check - if coil_combine ON, then need dataset
        # Error check - if ecc ON, then need dataset
        # Error check - if watref quant ON, then need dataset
        #
        # raise ValueError with msg and fail gracefully

        voxels = self.all_voxels
        nvox   = len(voxels)

        for i,voxel in enumerate(voxels):

            if statusbar: statusbar.SetStatusText(' Batch Fitting Voxel %d/%d ' % (i+1,nvox), 0)

            do_init = True if i == 0 else False     # triggers init of global vars in chain.run() if needed.

            for key in list(self.blocks.keys()):
                if key == 'spectral':
                    tmp = self.blocks['spectral'].chain.run([voxel], entry='all')
                    if 'fit' in list(self.blocks.keys()):
                        self.blocks['fit'].chain.run([voxel], entry='initial_only')
                        self.blocks['spectral'].set_do_fit(True, voxel)
                        tmp = self.blocks['spectral'].chain.run([voxel], entry='all')
                elif key == 'fit':
                    tmp = self.blocks["fit"].chain.run([voxel], entry='full_fit', do_init=do_init)
                else:
                    tmp = self.blocks[key].chain.run([voxel], entry='all')


    def get_associated_datasets(self, is_main_dataset=True):
        """
        NB. This is a convenience function only!  Not used in the deflate() 
            method because we need to fundge the uuid values in deflate.
            
        This method polls all blocks in this dataset to see if they rely on 
        another dataset. If so, the dataset object is stored in a list. This
        list is returned after all blocks are polled.
        
        """

        # In some cases a file may be associated twice. We only want to save once
        # so we gather all associated dataset and then filter for uniqueness.
        all_datasets    = []
        unique_datasets = []

        for block in self.blocks.values():
            # gather all associated datasets, unique or not
            all_datasets += block.get_associated_datasets(is_main_dataset)

        for item in all_datasets:
            # create list of unique associated datasets maintaining order but
            # also be sure that we do not include ourselves. 
            if item not in unique_datasets and item is not self:
                unique_datasets.append(item)

        return unique_datasets


    def set_associated_datasets(self, datasets):
        for block in self.blocks.values():
            block.set_associated_datasets(datasets)


    def set_associated_dataset_combine(self, dataset):

        if self.blocks["prep"].is_identity:
            raise ValueError("Identity block (Prep) can not set an associated dataset (combine)")
        else:
            the_dataset = dataset
            block       = the_dataset.blocks["prep"]
            block_raw   = the_dataset.blocks["raw"]
            if block.coil_combine_weights is None or block.coil_combine_phases is None:
                # this is a silent fail here, since assoc dataset should have same dims 
                return
            self.blocks["prep"].set.coil_combine_external_dataset    = the_dataset
            self.blocks["prep"].set.coil_combine_external_dataset_id = the_dataset.id
            self.blocks["prep"].set.coil_combine_external_filename   = block_raw.data_source
            self.blocks["prep"].coil_combine_weights                 = block.coil_combine_weights.copy()
            self.blocks["prep"].coil_combine_phases                  = block.coil_combine_phases.copy()


    def set_associated_dataset_ecc(self, dataset):

        if self.blocks["spectral"].is_identity:
            raise ValueError("Identity block (Spectral) can not set an associated dataset (ecc)")
        else:
            ecc_dataset = dataset
            block       = ecc_dataset.blocks["raw"]
            raw_data    = block.data.copy() * 0
            dims        = ecc_dataset.raw_dims
            for k in range(dims[3]):
                for j in range(dims[2]):
                    for i in range(dims[1]):
                        dat = block.data[k,j,i,:].copy() / block.data[k,j,i,0]
                        raw_data[k,j,i,:] = dat

            self.blocks["spectral"].set.ecc_dataset    = ecc_dataset
            self.blocks["spectral"].set.ecc_dataset_id = ecc_dataset.id
            self.blocks["spectral"].set.ecc_raw        = raw_data
            self.blocks["spectral"].set.ecc_filename   = block.data_source


    def set_associated_dataset_mmol(self, dataset):

        if self.blocks["fit"].is_identity:
            raise ValueError("Identity block (Fit) can not set an associated dataset (mmol)")
        else:
            mmol_dataset = dataset
            block        = mmol_dataset.blocks["raw"]
            self.blocks["fit"].set.macromol_model = constants.FitMacromoleculeMethod.SINGLE_BASIS_DATASET
            self.blocks["fit"].set.macromol_single_basis_dataset       = mmol_dataset
            self.blocks["fit"].set.macromol_single_basis_dataset_id    = mmol_dataset.id
            self.blocks["fit"].set.macromol_single_basis_dataset_fname = block.data_source


    def set_associated_dataset_quant(self, dataset):

        if self.blocks["quant"].is_identity:
            raise ValueError("Identity block (Quant) can not set an associated dataset (watref)")
        else:
            ref_dataset = dataset
            block       = ref_dataset.blocks["raw"]
            self.blocks["quant"].set.watref_dataset    = ref_dataset
            self.blocks["quant"].set.watref_dataset_id = ref_dataset.id
            self.blocks["quant"].set.watref_filename   = block.data_source


    def update_for_zerofill_change(self, zf_mult):
        """
        Here we assume that the dims coming in are for the new setting of
        the zero fill parameter. We check if we already match those values
        or if we need to reset block and chain dimensional results arrays

        """
        self.blocks["spectral"].set.zero_fill_multiplier = zf_mult
        # Let everyone know that the zerofill changed
        for block in self.blocks.values():
            block.set_dims(self)
        self.user_prior.basis.update(self)


    def update_for_preprocess_change(self):
        """
        Here we assume that the dims coming in are for the new setting of
        the zero fill parameter. We check if we already match those values
        or if we need to reset block and chain dimensional results arrays

        """
        # Let everyone know that the dims in preprocess block have changed
        for block in self.blocks.values():
            block.set_dims(self)


    def set_behave_as_preset(self, flag):
        """
        This will set all 'behave_as_preset' flags in the dataset and in
        the self.blocks list to the value of flag. User is responsible for
        setting/resetting these flags using this function.

        """
        val = flag == True
        self.behave_as_preset = flag
        for block in self.blocks.values():
            block.behave_as_preset = val


    def apply_preset(self, preset, voxel=(0,0,0), block_run=False, presetfile=''):
        '''
        Given a 'preset' dataset object (an actual dataset object that may
        or may not have data in it depending on whether it was saved as a
        preset file or dataset file), we extract the parameter settings for:

        - the user_prior object
        - each processing block and apply them to the current dataset
        - we ensure that the data dimensionality between blocks is properly
          maintained (e.g. zerofilling).
        - Finally, we run each block.process() method

        Things to know about Presets

        Each object in the presets blocks list (raw, prep, spectral, fit) is
        compared to the object class name in this dataset. If the names match
        the Settings object is copied over. If the class names do not match,
        no settings are copied over.

        The 'spectral' object has a few extra values copied over, like the
        phases and shift_frequencies.  Both the 'spectral' and 'fit' objects
        also need some run-time values recalculated after the settings are
        copied over.

        '''
        if self.blocks['raw'].__class__.__name__ == preset.blocks['raw'].__class__.__name__:
            self.blocks['raw'].set = copy.deepcopy(preset.blocks['raw'].set)

        if self.blocks['prep'].__class__.__name__ == preset.blocks['prep'].__class__.__name__:
            if not preset.blocks['prep'].is_identity:
                block = self.blocks['prep']
                block.set = copy.deepcopy(preset.blocks['prep'].set)
                block._reset_dimensional_data(self)

        if self.blocks['spectral'].__class__.__name__ == preset.blocks['spectral'].__class__.__name__:

            block = self.blocks['spectral']

            # We do a deep copy of all the settings from the preset dataset
            # into the current dataset, and check the result array dimensions

            block.set              = copy.deepcopy(preset.blocks['spectral'].set)
            block._phase_0         = copy.deepcopy(preset.blocks['spectral']._phase_0)
            block._phase_1         = copy.deepcopy(preset.blocks['spectral']._phase_1)
            block._frequency_shift = copy.deepcopy(preset.blocks['spectral']._frequency_shift)

            block.frequency_shift_lock = preset.blocks['spectral'].frequency_shift_lock
            block.phase_lock           = preset.blocks['spectral'].phase_lock
            block.phase_1_lock_at_zero = preset.blocks['spectral'].phase_1_lock_at_zero
#            block.left_shift_correct   = preset.blocks['spectral'].left_shift_correct
            block._reset_dimensional_data(self)

        if not preset.blocks['fit'].is_identity:

            # create fit object if it does not exist
            if   isinstance(preset.blocks['fit'], block_fit_voigt.BlockFitVoigt):
                self.add_voigt(force=True)
            elif isinstance(preset.blocks['fit'], block_fit_giso.BlockFitGiso):
                self.add_giso(force=True)

            # copy preset values into fit block and recalc as needed
            block = self.blocks['fit']
            block.set = copy.deepcopy(preset.blocks['fit'].set)
            prior = block.set.prior
            if   isinstance(preset.blocks['fit'], block_fit_voigt.BlockFitVoigt):
                prior.calculate_full_basis_set(block.set.prior_ppm_start, block.set.prior_ppm_end, self)
            elif isinstance(preset.blocks['fit'], block_fit_giso.BlockFitGiso):
                prior.calculate_full_basis_set(None, None, self)
            block._reset_dimensional_data(self)

        if not preset.blocks['quant'].is_identity:

            # create 'block_quant_watref' object if it does not exist
            self.add_watref(force=True)

            # copy preset values into watref block 
            block = self.blocks['quant']
            block.set = copy.deepcopy(preset.blocks['quant'].set)

        self.user_prior = copy.deepcopy(preset.user_prior)
        self.user_prior.basis.update(self)  # parent dataset may have different points/dwell


        self.preset_filename = presetfile


    def get_combo_results(self, voxel, quant=False):

        block = self.blocks["quant"] if quant else self.blocks["fit"]

        if block.is_identity:
            return None

        if quant:
            res = block.watref_results[:,voxel[0],voxel[1],voxel[2]]
        else:
            res = block.fit_results[:,voxel[0],voxel[1],voxel[2]]

        prior_list_unique = self.prior_list_unique

        combos = []

        all_combos = constants.FitPriorCalculateCombinations.choices
        for item in all_combos:
            met1, met2 = item.split('+')
            if met1 in prior_list_unique and met2 in prior_list_unique:
                imet1 = prior_list_unique.index(met1)
                imet2 = prior_list_unique.index(met2)
                combos.append([item, res[imet1]+res[imet2]])

        if combos == []:
            combos = None

        return combos


    def fit_results_as_html(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0,
                                  data_source="", image=None):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns HTML-formatted results for that voxel. The HTML is appropriate 
        for the wx.Html control (which understand limited HTML) as well as for
        writing to a file.

        If the image param is populated, it should be a tuple of 
        (mime_type, image_data). The former should be a string like "image/png".
        The latter should be base64-encoded image data.
        """
        fit = self.blocks["fit"]

        # First we assemble the data we need.
        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = fit.fit_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[:,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[:,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[:,voxel[0],voxel[1],voxel[2]]

        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        table1 = [['Area Results', 'Area', ' CrRao[%]', ' CnfInt[%]']]
        for i, item in enumerate(names):
            table1.append([item, res[i], crao[i], conf[i]])

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            table1.append(['MMol', res[nmet*2+4], 0.0, 0.0])

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel)
            if combo is not None:
                for item in combo:
                    table1.append([item[0], item[1], 0.0, 0.0])

        table1 = _pretty_space_table(table1, places=4)

        table2 = [['PPM Results', 'PPM', ' CrRao[ppm]', ' CnfInt[ppm]']]
        for i,item in enumerate(names):
            table2.append([item, res[i+nmet], crao[i+nmet], conf[i+nmet]])

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            table2.append(['MMol', res[nmet*2+5], 0.0, 0.0])

        table2 = _pretty_space_table(table2, places=4)

        table3 =     [['Global Results', 'Value', ' CrRao[delta]', ' CnfInt[%]']]
        table3.append(['Ta',     res[nmet*2+0], crao[nmet*2+0], conf[nmet*2+0] ])
        table3.append(['Tb',     res[nmet*2+1], crao[nmet*2+1], conf[nmet*2+1] ])
        table3.append(['Phase0', res[nmet*2+2], crao[nmet*2+2], conf[nmet*2+2] ])
        table3.append(['Phase1', res[nmet*2+3], crao[nmet*2+3], conf[nmet*2+3] ])
        table3 = _pretty_space_table(table3, places=5)

        table4 = [['Calculation Results', ' Value', '  Max LW', '  Min LW']]
        table4.append(['Linewidth', lw, lwmax, lwmin])
        table4.append(['ChiSquare', stats[0], ' ', ' '])
        table4.append(['Weighted ChiSquare', stats[1], ' ', ' '])
        matherr = str(stats[2] != 0)
        table4.append(['Math Finite Error', matherr, ' ', ' '])
        table4 = _pretty_space_table(table4, places=5)

        # Now that the data is assembled, we HTML-ify it.
        html = ElementTree.Element("html")
        head = ElementTree.SubElement(html, "head")
        style = util_xml.TextSubElement(head, "style", _CSS)
        style.set("type", "text/css")

        body = ElementTree.SubElement(html, "body")

        util_xml.TextSubElement(body, "h2", "Analysis Voigt Results")

        e_div = ElementTree.SubElement(body, "div")

        if data_source:
            e_tt = util_xml.TextSubElement(e_div, "tt", "Data Source: ")
            util_xml.TextSubElement(e_tt, "small", data_source)
            ElementTree.SubElement(e_div, "br")

        voxel = [x + 1 for x in voxel]
        util_xml.TextSubElement(e_div, "tt", 'Voxel: (%d,%d,%d)' % tuple(voxel))

        if image:
            # If there's image data, we assume that this will be written to 
            # a file for display in a proper browser, so we can use slightly
            # fancier HTML.
            mime_type, image_data = image

            e_div = ElementTree.SubElement(body, "div",
                                           { "id" : "image",
                                             "style" : "float: right; width: 50%",
                                           }
                                          )

            e_div.append(ElementTree.Comment(_IE_INCAPABLE_MSG))

            # In order to keep the HTML + image in one file, we use the
            # little-known "data" scheme.
            # ref: http://en.wikipedia.org/wiki/Data_URI_scheme
            src = "data:%s;base64,%s" % (mime_type, image_data)

            ElementTree.SubElement(e_div, "img",
                                           {"style" : "width: 90%",
                                            "src" : src
                                            })


        e_div = ElementTree.SubElement(body, "div", {"id" : "table"})

        tables = (table1, table2, table3, table4)

        for table in tables:
            title = table[0]
            e_pre = ElementTree.SubElement(e_div, "pre")
            e_u = ElementTree.SubElement(e_pre, "u")
            util_xml.TextSubElement(e_u, "b", title)
            e = util_xml.TextSubElement(e_div, "pre", '\n'.join(table[1:]))

        # Keep in mind that HTML is whitespace sensitive, and if you call 
        # util_xml.indent() on the HTML, it will change the formatting.

        return ElementTree.tostring(html)


    def fit_results_as_csv(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0, source="", dsetname="", nzfill=2, decor1=False):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns CSV-formatted (comma separated variables)string containing both
        the voxel fitting results and header string descriptions for each 
        column.

        if decor1 is set, metabolite abbreviations are added into the CrRao and CnfInt header strings

        """
        fit = self.blocks["fit"]

        hdr = []
        val = []

        hdr.append('Filename')
        source = source.replace(",","_")     # some users have commas in filenames 
        val.append(source)

        hdr.append('Dataset Name')
        dsetname = dsetname.replace(",","_") # some users have commas in filenames    
        val.append(dsetname)

        hdr.append('Voxel')
        val.append(str(voxel[0]).zfill(nzfill)+' '+str(voxel[1]).zfill(nzfill)+' '+str(voxel[2]).zfill(nzfill))

        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = fit.fit_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[ :,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[ :,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[  :,voxel[0],voxel[1],voxel[2]]

        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        for i, item in enumerate(names):
            addstr = '' if not decor1 else ' '+item
            hdr.append('Area '+item)
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[i]))
            val.append(str(crao[i]))
            val.append(str(conf[i]))

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            addstr = '' if not decor1 else ' MMol'
            hdr.append('Area MMol')
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[nmet*2+4]))
            val.append(str(0.0))
            val.append(str(0.0))

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel)
            if combo is not None:
                for item in combo:
                    addstr = '' if not decor1 else ' '+item[0]
                    hdr.append('Area '+item[0])
                    hdr.append('CrRao[%]'+addstr)
                    hdr.append('CnfInt[%]'+addstr)
                    val.append(str(item[1]))
                    val.append(str(0.0))
                    val.append(str(0.0))

        for i,item in enumerate(names):
            addstr = '' if not decor1 else ' '+item
            hdr.append('PPM '+item)
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[i+nmet]))
            val.append(str(crao[i+nmet]))
            val.append(str(conf[i+nmet]))

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            addstr = '' if not decor1 else ' MMol'
            hdr.append('PPM MMol')
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[nmet*2+4]))
            val.append(str(0.0))
            val.append(str(0.0))


        hdr.append('Ta ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+0]))
        val.append(str(crao[nmet*2+0]))
        val.append(str(conf[nmet*2+0]))

        hdr.append('Tb ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+1]))
        val.append(str(crao[nmet*2+1]))
        val.append(str(conf[nmet*2+1]))

        hdr.append('Phase0 ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+2]))
        val.append(str(crao[nmet*2+2]))
        val.append(str(conf[nmet*2+2]))

        hdr.append('Phase1 ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+3]))
        val.append(str(crao[nmet*2+3]))
        val.append(str(conf[nmet*2+3]))

        hdr.append('Linewidth ')
        hdr.append('Max LW')
        hdr.append('Min LW')
        val.append(str(lw))
        val.append(str(lwmax))
        val.append(str(lwmin))

        hdr.append('ChiSquare ')
        val.append(str(stats[0]))

        hdr.append('WtChiSquare ')
        val.append(str(stats[1]))

        hdr.append('Math Finite Error ')
        matherr = str(stats[2] != 0)
        val.append(str(matherr))

        return val, hdr


    def fit_results_in_table(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0,
                                   nozeros=False, noppm=False, fixphase=False,
                                   no_conf=False, places=5, pad=2,
                                   short_form=False, format_float=False):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns formatted results for that voxel. 
        
        """
        fit = self.blocks["fit"]

        # First we assemble the data we need.
        nmet  = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = fit.fit_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[:,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[:,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[:,voxel[0],voxel[1],voxel[2]]

        do_crao = False if max(crao) == 0.0 and nozeros else True

        do_conf = False if max(conf) == 0.0 and nozeros else True
        do_conf = do_conf if no_conf == False else False

        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        if format_float:
            nmet = len(fit.set.prior_list)
            fmt_area = '%.3f' if max(res[0:nmet]) > 0.01 else '%.6f'
            fmt_arcr = '%.3f'
            fmt_ppm  = '%.3f'
            fmt_ppcr = '%.5f'


        hdr1 = ['Metab Results', 'Area']
        hdr2 = ['PPM Results', 'PPM']
        hdr3 = ['Global Results', 'Value']
        hdr4 = ['Calc Results', ' Value']
        if do_crao:
            hdr1.append('CrRao[%]')
            hdr2.append('CrRao[delta]')
            hdr3.append('CrRao[delta]')
            hdr4.append(' ')
        if do_conf:
            hdr1.append('CnfInt[%]')
            hdr2.append('CnfInt[delta]')
            hdr3.append('CnfInt[delta]')
            hdr4.append(' ')

        nsect = [0,]
        table1 = [hdr1,]

        for i, item in enumerate(names):
            if format_float:
                tmp = [item, fmt_area % res[i]]
                if do_crao: tmp.append(fmt_arcr % crao[i])
                if do_conf: tmp.append(fmt_arcr % conf[i])
            else:
                tmp = [item, res[i]]
                if do_crao: tmp.append(crao[i])
                if do_conf: tmp.append(conf[i])
            table1.append(tmp)

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            if format_float:
                tmp = ['MMol', fmt_area % res[nmet*2+4]]
                if do_crao: tmp.append(fmt_arcr % 0.0)
                if do_conf: tmp.append(fmt_arcr % 0.0)
            else:
                tmp = ['MMol', res[nmet*2+4]]
                if do_crao: tmp.append(0.0)
                if do_conf: tmp.append(0.0)
            table1.append(tmp)

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel)
            if combo is not None:
                for item in combo:
                    if format_float:
                        tmp = [item[0], fmt_area % item[1]]
                        if do_crao: tmp.append(fmt_arcr % 0.0)
                        if do_conf: tmp.append(fmt_arcr % 0.0)
                    else:
                        tmp = [item[0], item[1]]
                        if do_crao: tmp.append(0.0)
                        if do_conf: tmp.append(0.0)
                    table1.append(tmp)

        nsect.append(len(table1))

        if not noppm:
            table1.append(hdr2)
            for i,item in enumerate(names):
                if format_float:
                    tmp = [item, fmt_ppm % res[i+nmet]]
                    if do_crao: tmp.append(fmt_ppcr % crao[i+nmet])
                    if do_conf: tmp.append(fmt_ppcr % conf[i+nmet])
                else:
                    tmp = [item, res[i+nmet]]
                    if do_crao: tmp.append(crao[i+nmet])
                    if do_conf: tmp.append(conf[i+nmet])
                table1.append(tmp)

            if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
                if format_float:
                    tmp = ['MMol', fmt_ppm % res[nmet*2+5]]
                    if do_crao: tmp.append(fmt_ppcr % 0.0)
                    if do_conf: tmp.append(fmt_ppcr % 0.0)
                else:
                    tmp = ['MMol', res[nmet*2+5]]
                    if do_crao: tmp.append(0.0)
                    if do_conf: tmp.append(0.0)

                table1.append(tmp)

        nsect.append(len(table1))
        table1.append(hdr3)

        tmp = ['Ta', res[nmet*2+0]]
        if do_crao: tmp.append(crao[nmet*2+0])
        if do_conf: tmp.append(conf[nmet*2+0])
        table1.append(tmp)

        tmp = ['Tb', res[nmet*2+1]]
        if do_crao: tmp.append(crao[nmet*2+1])
        if do_conf: tmp.append(conf[nmet*2+1])
        table1.append(tmp)

        if not fixphase:
            tmp = ['Phase0', res[nmet*2+2]]
            if do_crao: tmp.append(crao[nmet*2+2])
            if do_conf: tmp.append(conf[nmet*2+2])
            table1.append(tmp)

            tmp = ['Phase1', res[nmet*2+3]]
            if do_crao: tmp.append(crao[nmet*2+3])
            if do_conf: tmp.append(conf[nmet*2+3])
            table1.append(tmp)
        else:
            tmp = ['Phase0', '0.0 ('+str(np.round(res[nmet*2+2],1))+')']
            if do_crao: tmp.append(crao[nmet*2+2])
            if do_conf: tmp.append(conf[nmet*2+2])
            table1.append(tmp)

            tmp = ['Phase1', '0.0 ('+str(np.round(res[nmet*2+3],1))+')']
            if do_crao: tmp.append(crao[nmet*2+3])
            if do_conf: tmp.append(conf[nmet*2+3])
            table1.append(tmp)

        nsect.append(len(table1))
        table1.append(hdr4)

        tmp = ['Linewidth', lw]
        if do_crao: tmp.append(' ')
        if do_conf: tmp.append(' ')
        table1.append(tmp)

        if not short_form:
            tmp = ['ChiSquare', stats[0]]
            if do_crao: tmp.append(' ')
            if do_conf: tmp.append(' ')
            table1.append(tmp)

        tmp = ['Weighted ChiSquare', stats[1]]
        if do_crao: tmp.append(' ')
        if do_conf: tmp.append(' ')
        table1.append(tmp)

        if not short_form:
            matherr = str(stats[2] != 0)
            tmp = ['Math Finite Error', matherr]
            if do_crao: tmp.append(' ')
            if do_conf: tmp.append(' ')
            table1.append(tmp)

        table1 = _pretty_space_table_inplace(table1, places=places, pad=pad)

        return table1, nsect


    def debug_results_as_csv(self, voxel, source="", dsetname="", nzfill=2, decor1=False):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns CSV-formatted (comma separated variables)string containing both
        the voxel fitting results and header string descriptions for each 
        column.

        if decor1 is set, metabolite abbreviations are added into the CrRao and CnfInt header strings

        """
        fit = self.blocks["fit"]

        hdr = []
        val = []

        hdr.append('Filename')
        source = source.replace(",","_")     # some users have commas in filenames 
        val.append(source)

        hdr.append('Dataset Name')
        dsetname = dsetname.replace(",","_") # some users have commas in filenames    
        val.append(dsetname)

        hdr.append('Voxel')
        val.append(str(voxel[0]).zfill(nzfill)+' '+str(voxel[1]).zfill(nzfill)+' '+str(voxel[2]).zfill(nzfill))

        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = fit.fit_results[  :,voxel[0],voxel[1],voxel[2]]
        init  = fit.initial_values[:,voxel[0],voxel[1],voxel[2]]


        for i, item in enumerate(names):
            addstr = '' if not decor1 else ' '+item
            hdr.append('Area '+item)
            hdr.append('Init '+addstr)
            hdr.append('Ratio [%]'+addstr)

            val.append("{:.2f}".format(res[i]) )
            val.append("{:.2f}".format(init[i]) )
            val.append("{:.2f}".format(100.0*res[i]/init[i])  )


        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            addstr = '' if not decor1 else ' MMol'
            hdr.append('Area MMol')
            hdr.append('Init'+addstr)
            hdr.append('Ratio [%]'+addstr)

            val.append("{:.2f}".format(res[nmet*2+4]) )
            val.append("{:.2f}".format(init[nmet*2+4]) )
            val.append("{:.2f}".format(100.0*res[nmet*2+4]/init[nmet*2+4])  )

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel)
            if combo is not None:
                for item in combo:
                    addstr = '' if not decor1 else ' '+item[0]
                    hdr.append('Area '+item[0])
                    hdr.append('Init'+addstr)
                    hdr.append('Ratio[%]'+addstr)

                    val.append("{:.2f}".format(item[1]) )
                    val.append("{:.2f}".format(0.0) )
                    val.append("{:.2f}".format(0.0) )


        for i,item in enumerate(names):
            addstr = '' if not decor1 else ' '+item
            hdr.append('PPM '+item)
            hdr.append('Init'+addstr)
            hdr.append('Ratio[%]'+addstr)

            val.append("{:.2f}".format(res[i+nmet]) )
            val.append("{:.2f}".format(init[i+nmet]) )
            val.append("{:.2f}".format(100.0*res[i+nmet]/init[i+nmet])  )


        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            addstr = '' if not decor1 else ' MMol'
            hdr.append('PPM MMol')
            hdr.append('Init'+addstr)
            hdr.append('Ratio[%]'+addstr)

            val.append("{:.2f}".format(res[nmet*2+4]) )
            val.append("{:.2f}".format(init[nmet*2+4]) )
            val.append("{:.2f}".format(100.0*res[nmet*2+4]/init[nmet*2+4])  )


        hdr.append('Ta ')
        hdr.append('Init')
        hdr.append('Ratio[%]')
        val.append("{:.2f}".format(res[nmet*2+0]) )
        val.append("{:.2f}".format(init[nmet*2+0]) )
        val.append("{:.2f}".format(100.0*res[nmet*2+0]/init[nmet*2+0])  )

        hdr.append('Tb ')
        hdr.append('Init')
        hdr.append('Ratio[%]')
        val.append("{:.2f}".format(res[nmet*2+1]) )
        val.append("{:.2f}".format(init[nmet*2+1]) )
        val.append("{:.2f}".format(100.0*res[nmet*2+1]/init[nmet*2+1])  )

        hdr.append('Phase0 ')
        hdr.append('Init')
        hdr.append('Ratio[%]')
        val.append("{:.2f}".format(res[nmet*2+2]) )
        val.append("{:.2f}".format(init[nmet*2+2]) )
        val.append("{:.2f}".format(100.0*res[nmet*2+2]/init[nmet*2+2])  )

        hdr.append('Phase1 ')
        hdr.append('Init')
        hdr.append('Ratio[%]')
        val.append("{:.2f}".format(res[nmet*2+3]) )
        val.append("{:.2f}".format(init[nmet*2+3]) )
        val.append("{:.2f}".format(100.0*res[nmet*2+3]/init[nmet*2+3])  )

        return val, hdr


    def debug_results_as_popup(self, voxel, source="", dsetname="", nzfill=2):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns CSV-formatted (comma separated variables)string containing both
        the voxel fitting results and header string descriptions for each 
        column.

        if decor1 is set, metabolite abbreviations are added into the CrRao and CnfInt header strings

        """
        fit = self.blocks["fit"]

        lines = []

        lines.append('Filename     - '+source.replace(",","_"))     # some users have commas in filenames 
        lines.append('Dataset Name - '+dsetname.replace(",","_"))   # some users have commas in filenames        
        lines.append('Voxel        - '+str(voxel[0]).zfill(nzfill)+' '+str(voxel[1]).zfill(nzfill)+' '+str(voxel[2]).zfill(nzfill))
        lines.append(' ')
        lines.append("optimize_bounds_area_max          - "+str(fit.set.optimize_bounds_area_max) )
        lines.append("optimize_bounds_area_min          - "+str(fit.set.optimize_bounds_area_min) )
        lines.append("optimize_bounds_area_max_small    - "+str(fit.set.optimize_bounds_area_max_small) )
        lines.append("optimize_bounds_area_min_small    - "+str(fit.set.optimize_bounds_area_min_small) )
        lines.append("optimize_enable_bounds_area_small - "+str(fit.set.optimize_enable_bounds_area_small) )
        lines.append("optimize_bounds_range_ppm         - "+str(fit.set.optimize_bounds_range_ppm) )
        lines.append("optimize_bounds_range_ppm_small   - "+str(fit.set.optimize_bounds_range_ppm_small) )
        lines.append("optimize_enable_bounds_ppm_small  - "+str(fit.set.optimize_enable_bounds_ppm_small) )
        lines.append(' ')

        nmet  = len(fit.set.prior_list)
        names = fit.set.prior_list
        res   = fit.fit_results[   :,voxel[0],voxel[1],voxel[2]]
        init  = fit.initial_values[:,voxel[0],voxel[1],voxel[2]]


        # Add a header for the results and initial values columns
        prefix_metab = 'Area '
        prefix_hdr   = 'Metabolite '

        width = max([len(name) for name in names])
        pad   = width + len(prefix_metab) - len(prefix_hdr)
        pad   = pad if pad>0 else 0
        lines.append(prefix_hdr+pad*' '+'\t'+'Result\t'+'Init\t'+'Ratio')

        for i, name in enumerate(names):

            line = prefix_metab+name.ljust(width)+'\t'
            line += "{:.2f}".format(res[i])+'\t'
            line += "{:.2f}".format(init[i])+'\t'
            line += "{:.2f}".format(100.0*res[i]/init[i])+'% \t'
            lines.append(line)

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:

            line  = prefix_metab+'MMol'.ljust(width)+'\t'
            line += "{:.2f}".format(res[nmet*2+4])+'\t'
            line += "{:.2f}".format(init[nmet*2+4])+'\t'
            line += "{:.2f}".format(100.0*res[nmet*2+4]/init[nmet*2+4])+'% \t'
            lines.append(line)

        return lines


    def quant_results_as_csv_vert_col_for_ismrm(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0, source="", dsetname=""):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns CSV-formatted (comma separated variables)string containing both
        the voxel fitting results and header string descriptions for each 
        column.

        """
        fit   = self.blocks["fit"]
        quant = self.blocks["quant"]

        hdr = []
        val = []

        dsetname = dsetname.replace(",","_") # some users have commas in filenames    
        path, fname = os.path.split(dsetname)

        hdr.append('Dataset Name')
        val.append(fname+", ")

        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = quant.watref_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[ :,voxel[0],voxel[1],voxel[2]]

        if len(crao) != len(res):
            crao = res * 0

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            hdr.append('MMBL')
            val.append(str(res[nmet*2+4])+", 0.0")

        for i, item in enumerate(names):
            hdr.append(item)
            val.append(str(res[i]*1e6)+","+str(crao[i]))

        hdr.append('Linewidth ')
        val.append(str(lw)+", 0.0")

        return val, hdr


    def quant_results_as_csv(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0, source="", dsetname="", nzfill=2, decor1=False):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns CSV-formatted (comma separated variables)string containing both
        the voxel fitting results and header string descriptions for each 
        column.

        """
        fit   = self.blocks["fit"]
        quant = self.blocks["quant"]

        hdr = []
        val = []

        hdr.append('Filename')
        source = source.replace(",","_")     # some users have commas in filenames 
        val.append(source)

        hdr.append('Dataset Name')
        dsetname = dsetname.replace(",","_") # some users have commas in filenames    
        val.append(dsetname)

        hdr.append('Voxel')
        val.append(str(voxel[0]).zfill(nzfill)+' '+str(voxel[1]).zfill(nzfill)+' '+str(voxel[2]).zfill(nzfill))

        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = quant.watref_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[ :,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[ :,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[  :,voxel[0],voxel[1],voxel[2]]
        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        for i, item in enumerate(names):
            addstr = ''
            if decor1:
                addstr = ' '+item
            hdr.append('Conc [mM]'+item)
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[i]))
            val.append(str(crao[i]))
            val.append(str(conf[i]))

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            addstr = '' if not decor1 else ' MMol'
            hdr.append('Conc [mM] MMol')
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[nmet*2+4]))
            val.append(str(0.0))
            val.append(str(0.0))

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel, quant=True)
            if combo is not None:
                for item in combo:
                    addstr = '' if not decor1 else ' '+item[0]
                    hdr.append('Conc [mM] '+item[0])
                    hdr.append('CrRao[%]'+addstr)
                    hdr.append('CnfInt[%]'+addstr)
                    val.append(str(item[1]))
                    val.append(str(0.0))
                    val.append(str(0.0))

        for i,item in enumerate(names):
            addstr = ''
            if decor1:
                addstr = ' '+item
            hdr.append('PPM '+item)
            hdr.append('CrRao[%]'+addstr)
            hdr.append('CnfInt[%]'+addstr)
            val.append(str(res[i+nmet]))
            val.append(str(crao[i+nmet]))
            val.append(str(conf[i+nmet]))

        hdr.append('Ta ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+0]))
        val.append(str(crao[nmet*2+0]))
        val.append(str(conf[nmet*2+0]))

        hdr.append('Tb ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+1]))
        val.append(str(crao[nmet*2+1]))
        val.append(str(conf[nmet*2+1]))

        hdr.append('Phase0 ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+2]))
        val.append(str(crao[nmet*2+2]))
        val.append(str(conf[nmet*2+2]))

        hdr.append('Phase1 ')
        hdr.append('CrRao[%]')
        hdr.append('CnfInt[%]')
        val.append(str(res[nmet*2+3]))
        val.append(str(crao[nmet*2+3]))
        val.append(str(conf[nmet*2+3]))

        hdr.append('Linewidth ')
        hdr.append('Max LW')
        hdr.append('Min LW')
        val.append(str(lw))
        val.append(str(lwmax))
        val.append(str(lwmin))

        hdr.append('ChiSquare ')
        val.append(str(stats[0]))

        hdr.append('WtChiSquare ')
        val.append(str(stats[1]))

        hdr.append('Math Finite Error ')
        matherr = str(stats[2] != 0)
        val.append(str(matherr))

        return val, hdr



    def quant_results_as_html(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0,
                                   data_source="", image=None):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns HTML-formatted results for that voxel. The HTML is appropriate 
        for the wx.Html control (which understand limited HTML) as well as for
        writing to a file.

        If the image param is populated, it should be a tuple of 
        (mime_type, image_data). The former should be a string like "image/png".
        The latter should be base64-encoded image data.
        """
        fit   = self.blocks["fit"]
        quant = self.blocks["quant"]

        # First we assemble the data we need.
        nmet = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = quant.watref_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[:,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[:,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[:,voxel[0],voxel[1],voxel[2]]

        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        table1 = [['Quant Results', 'Conc [mM]', ' CrRao[%]', ' CnfInt[%]']]
        for i, item in enumerate(names):
            table1.append([item, res[i], crao[i], conf[i]])

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            table1.append(['MMol', res[nmet*2+4], 0.0, 0.0])

        if fit.set.prior_calculate_combinations:
            combo = self.get_combo_results(voxel, quant=True)
            if combo is not None:
                for item in combo:
                    table1.append([item[0], item[1], 0.0, 0.0])

        table1 = _pretty_space_table(table1, places=4)

        table2 = [['PPM Results', 'PPM', ' CrRao[ppm]', ' CnfInt[ppm]']]
        for i,item in enumerate(names):
            table2.append([item, res[i+nmet], crao[i+nmet], conf[i+nmet]])

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            table2.append(['MMol', res[nmet*2+5], 0.0, 0.0])

        table2 = _pretty_space_table(table2, places=4)

        table3 =     [['Global Results', 'Value', ' CrRao[delta]', ' CnfInt[%]']]
        table3.append(['Ta',     res[nmet*2+0], crao[nmet*2+0], conf[nmet*2+0] ])
        table3.append(['Tb',     res[nmet*2+1], crao[nmet*2+1], conf[nmet*2+1] ])
        table3.append(['Phase0', res[nmet*2+2], crao[nmet*2+2], conf[nmet*2+2] ])
        table3.append(['Phase1', res[nmet*2+3], crao[nmet*2+3], conf[nmet*2+3] ])
        table3 = _pretty_space_table(table3, places=5)

        table4 = [['Calculation Results', ' Value', '  Max LW', '  Min LW']]
        table4.append(['Linewidth', lw, lwmax, lwmin])
        table4.append(['ChiSquare', stats[0], ' ', ' '])
        table4.append(['Weighted ChiSquare', stats[1], ' ', ' '])
        matherr = str(stats[2] != 0)
        table4.append(['Math Finite Error', matherr, ' ', ' '])
        table4 = _pretty_space_table(table4, places=5)

        # Now that the data is assembled, we HTML-ify it.
        html = ElementTree.Element("html")
        head = ElementTree.SubElement(html, "head")
        style = util_xml.TextSubElement(head, "style", _CSS)
        style.set("type", "text/css")

        body = ElementTree.SubElement(html, "body")

        util_xml.TextSubElement(body, "h2", "Analysis Results - Water Reference Quantitation")

        e_div = ElementTree.SubElement(body, "div")

        if data_source:
            e_tt = util_xml.TextSubElement(e_div, "tt", "Data Source: ")
            util_xml.TextSubElement(e_tt, "small", data_source)
            ElementTree.SubElement(e_div, "br")

        voxel = [x + 1 for x in voxel]
        util_xml.TextSubElement(e_div, "tt", 'Voxel: (%d,%d,%d)' % tuple(voxel))

        if image:
            # If there's image data, we assume that this will be written to 
            # a file for display in a proper browser, so we can use slightly
            # fancier HTML.
            mime_type, image_data = image

            e_div = ElementTree.SubElement(body, "div",
                                           { "id" : "image",
                                             "style" : "float: right; width: 50%",
                                           }
                                          )

            e_div.append(ElementTree.Comment(_IE_INCAPABLE_MSG))

            # In order to keep the HTML + image in one file, we use the
            # little-known "data" scheme.
            # ref: http://en.wikipedia.org/wiki/Data_URI_scheme
            src = "data:%s;base64,%s" % (mime_type, image_data)

            ElementTree.SubElement(e_div, "img",
                                           {"style" : "width: 90%",
                                            "src" : src
                                            })


        e_div = ElementTree.SubElement(body, "div", {"id" : "table"})

        tables = (table1, table2, table3, table4)

        for table in tables:
            title = table[0]
            e_pre = ElementTree.SubElement(e_div, "pre")
            e_u = ElementTree.SubElement(e_pre, "u")
            util_xml.TextSubElement(e_u, "b", title)
            e = util_xml.TextSubElement(e_div, "pre", '\n'.join(table[1:]))

        # Keep in mind that HTML is whitespace sensitive, and if you call 
        # util_xml.indent() on the HTML, it will change the formatting.

        return ElementTree.tostring(html)


    def quant_results_in_table(self, voxel, lw=0.0, lwmin=0.0, lwmax=0.0,
                                     nozeros=False, noppm=False, fixphase=False,
                                     no_conf=False, places=5, pad=2,
                                     short_form=False, format_float=False):
        """
        Given a voxel, linewidth params, and a data source (often a filename), 
        returns formatted results for that voxel. 
        
        """
        fit   = self.blocks["fit"]
        quant = self.blocks["quant"]

        # First we assemble the data we need.
        nmet  = len(fit.set.prior_list)

        names = fit.set.prior_list
        res   = quant.watref_results[:,voxel[0],voxel[1],voxel[2]]
        crao  = fit.cramer_rao[ :,voxel[0],voxel[1],voxel[2]]
        conf  = fit.confidence[ :,voxel[0],voxel[1],voxel[2]]
        stats = fit.fit_stats[  :,voxel[0],voxel[1],voxel[2]]

        do_crao = False if max(crao) == 0.0 and nozeros else True

        do_conf = False if max(conf) == 0.0 and nozeros else True
        do_conf = do_conf if no_conf == False else False

        # both cramer-rao and confidence intervals may be off/on
        if len(crao) != len(res):
            crao = res * 0
        if len(conf) != len(res):
            conf = res * 0

        if format_float:
            nmet = len(fit.set.prior_list)
            fmt_area = '%.3f' if max(res[0:nmet]) > 0.01 else '%.6f'
            fmt_arcr = '%.3f'
            fmt_ppm  = '%.3f'
            fmt_ppcr = '%.5f'

        hdr1 = ['Metab Results', 'Conc[mM]']
        hdr2 = ['PPM Results', 'PPM']
        hdr3 = ['Global Results', 'Value']
        hdr4 = ['Calc Results', ' Value']
        if do_crao:
            hdr1.append('CrRao[%]')
            hdr2.append('CrRao[delta]')
            hdr3.append('CrRao[delta]')
            hdr4.append(' ')
        if do_conf:
            hdr1.append('CnfInt[%]')
            hdr2.append('CnfInt[delta]')
            hdr3.append('CnfInt[delta]')
            hdr4.append(' ')

        nsect = [0,]
        table1 = [hdr1,]

        for i, item in enumerate(names):
            if format_float:
                tmp = [item, fmt_area % res[i]]
                if do_crao: tmp.append(fmt_arcr % crao[i])
                if do_conf: tmp.append(fmt_arcr % conf[i])
            else:
                tmp = [item, res[i]]
                if do_crao: tmp.append(crao[i])
                if do_conf: tmp.append(conf[i])
            table1.append(tmp)

        if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
            if format_float:
                tmp = ['MMol', fmt_area % res[nmet*2+4]]
                if do_crao: tmp.append(fmt_arcr % 0.0)
                if do_conf: tmp.append(fmt_arcr % 0.0)
            else:
                tmp = ['MMol', res[nmet*2+4]]
                if do_crao: tmp.append(0.0)
                if do_conf: tmp.append(0.0)
            table1.append(tmp)

        if fit.set.prior_calculate_combinations:
            #combo = self.get_combo_results(voxel)
            combo = self.get_combo_results(voxel, quant=True)
            if combo is not None:
                for item in combo:
                    if format_float:
                        tmp = [item[0], fmt_area % item[1]]
                        if do_crao: tmp.append(fmt_arcr % 0.0)
                        if do_conf: tmp.append(fmt_arcr % 0.0)
                    else:
                        tmp = [item[0], item[1]]
                        if do_crao: tmp.append(0.0)
                        if do_conf: tmp.append(0.0)
                    table1.append(tmp)

        nsect.append(len(table1))

        if not noppm:
            table1.append(hdr2)
            for i,item in enumerate(names):
                if format_float:
                    tmp = [item, fmt_ppm % res[i+nmet]]
                    if do_crao: tmp.append(fmt_ppcr % crao[i+nmet])
                    if do_conf: tmp.append(fmt_ppcr % conf[i+nmet])
                else:
                    tmp = [item, res[i+nmet]]
                    if do_crao: tmp.append(crao[i+nmet])
                    if do_conf: tmp.append(conf[i+nmet])
                table1.append(tmp)

            if fit.set.macromol_model == FitMacromoleculeMethod.SINGLE_BASIS_DATASET:
                if format_float:
                    tmp = ['MMol', fmt_ppm % res[nmet*2+5]]
                    if do_crao: tmp.append(fmt_ppcr % 0.0)
                    if do_conf: tmp.append(fmt_ppcr % 0.0)
                else:
                    tmp = ['MMol', res[nmet*2+5]]
                    if do_crao: tmp.append(0.0)
                    if do_conf: tmp.append(0.0)

                table1.append(tmp)

        nsect.append(len(table1))
        table1.append(hdr3)

        tmp = ['Ta', res[nmet*2+0]]
        if do_crao: tmp.append(crao[nmet*2+0])
        if do_conf: tmp.append(conf[nmet*2+0])
        table1.append(tmp)

        tmp = ['Tb', res[nmet*2+1]]
        if do_crao: tmp.append(crao[nmet*2+1])
        if do_conf: tmp.append(conf[nmet*2+1])
        table1.append(tmp)

        if not fixphase:
            tmp = ['Phase0', res[nmet*2+2]]
            if do_crao: tmp.append(crao[nmet*2+2])
            if do_conf: tmp.append(conf[nmet*2+2])
            table1.append(tmp)

            tmp = ['Phase1', res[nmet*2+3]]
            if do_crao: tmp.append(crao[nmet*2+3])
            if do_conf: tmp.append(conf[nmet*2+3])
            table1.append(tmp)
        else:
            tmp = ['Phase0', '0.0 ('+str(np.round(res[nmet*2+2],1))+')']
            if do_crao: tmp.append(crao[nmet*2+2])
            if do_conf: tmp.append(conf[nmet*2+2])
            table1.append(tmp)

            tmp = ['Phase1', '0.0 ('+str(np.round(res[nmet*2+3],1))+')']
            if do_crao: tmp.append(crao[nmet*2+3])
            if do_conf: tmp.append(conf[nmet*2+3])
            table1.append(tmp)

        nsect.append(len(table1))
        table1.append(hdr4)

        tmp = ['Linewidth', lw]
        if do_crao: tmp.append(' ')
        if do_conf: tmp.append(' ')
        table1.append(tmp)

        if not short_form:
            tmp = ['ChiSquare', stats[0]]
            if do_crao: tmp.append(' ')
            if do_conf: tmp.append(' ')
            table1.append(tmp)

        tmp = ['Weighted ChiSquare', stats[1]]
        if do_crao: tmp.append(' ')
        if do_conf: tmp.append(' ')
        table1.append(tmp)

        if not short_form:
            matherr = str(stats[2] != 0)
            tmp = ['Math Finite Error', matherr]
            if do_crao: tmp.append(' ')
            if do_conf: tmp.append(' ')
            table1.append(tmp)

        table1 = _pretty_space_table_inplace(table1, places=places, pad=pad)

        return table1, nsect







########################   Inflate()/Deflate()    ########################


    def deflate(self, flavor=Deflate.ETREE, is_main_dataset=True):

        associated      = []
        unique_datasets = []

        if is_main_dataset:
            # This documentation needs to be re-edited for brevity. Most important
            # point here is that this logic is only run in 'deflate' for the main
            # dataset. All other assoc datasets or sub-datasets are accessed using
            # the mrs_dataset.get_associated_dataset() call. That call will bore
            # down but will NOT change uuid values. That is why we change uuid
            # values in all unique_datasets below BEFORE deflating them.
            #
            # associated datasets are gathered here so that they can be returned as
            # a result of this deflate in a list, they are positioned first in the
            # list that is returned to ensure that they are available for inflate
            # prior to the inflate of the main data set with which they associate.
            #
            # In some cases a file may be associated twice, as in the case of GE
            # Pfiles that associate water and water suppressed in both raw and
            # possibly the ECC filter in Spectral tab. We only want to save once
            # so we gather all associated datasets, filter for uniqueness, change
            # the id, deflate it then change the id back for the currently open
            # dataset.
            all_datasets = []

            for block in self.blocks.values():
                # gather all associated datasets, unique or not
                all_datasets += block.get_associated_datasets(is_main_dataset)

            for item in all_datasets:
                # create list of unique associated datasets maintaining order but
                # also be sure that we do not include ourselves. This could occur
                # in Edited datasets which have on/off/add/sub states.
                if item not in unique_datasets and item is not self:
                    unique_datasets.append(item)

            # rename all dataset IDs if main dataset
            for dataset in unique_datasets:
                # As we save these associated datasets, we want to use unique
                # ids that won't collide with ids in memory now, should they
                # be loaded right back in again. So here the dataset.id is changed
                # and deflated. Then all the rest of the main dataset (including
                # any refs to the associated datasets) are deflated. Then the
                # associated datasets ids are restored

                dataset.id_saved = dataset.id
                dataset.id = util_misc.uuid()

            # save all associated datasets if main
            for dataset in unique_datasets:
                # FIXME - BJS test if this is a true thing, we do NOT want
                #   to save any associated datasets when we do presets save
                if not self.behave_as_preset:
                    associated.append(dataset.deflate(flavor, is_main_dataset=False))


        if flavor == Deflate.ETREE:
            e = ElementTree.Element("dataset",
                                      { "id" : self.id,
                                        "version" : self.XML_VERSION})

            if self.behave_as_preset:
                util_xml.TextSubElement(e, "behave_as_preset", self.behave_as_preset)

            util_xml.TextSubElement(e, "preset_filename", self.preset_filename)

            e.append(self.user_prior.deflate())

            ee = ElementTree.SubElement(e, "blocks")

            for block in self.blocks.values():
                if not block.is_identity:
                    ee.append(block.deflate())
                #else:
                    # We don't clutter up the XML with identity blocks.

            if associated and is_main_dataset:
                # e goes at the end here so that later when we reload this dataset, 
                # all assoc datasets are loaded in before we try to load this 'main' one
                associated.append(e)
                e = associated

            return e

        elif flavor == Deflate.DICTIONARY:
            return self.__dict__.copy()

        if is_main_dataset:
            for dataset in unique_datasets:
                # Restore the original UUID in associated datasets
                dataset.id = dataset.id_saved


    def inflate(self, source):
        if hasattr(source, "makeelement"):
            # Quacks like an ElementTree.Element
            xml_version = source.get("version")
            self.id = source.get("id")

            val = source.findtext("behave_as_preset")       # default is False
            if val is not None:
                self.behave_as_preset = util_xml.BOOLEANS[val]

            for name in ("preset_filename",):
                setattr(self, name, source.findtext(name))      # this code allows 'None' value

            #==================================================================
            # This code chunk deals with earlier version datasets that need
            # to be parsed into the current layout of the dataset object
            #==================================================================

            if xml_version == "1.0.0":
                # For this version, user prior information is extracted from
                # the basic block (below).
                pass
            else:
                # In all other versions, user prior resides in its own node.
                self.user_prior.inflate(source.find("user_prior"))

            # The individual <block> elements are wrapped in a <blocks>
            # element. The <blocks> element has no attributes and its only
            # children are <block> elements. <blocks> is just a wrapper.
            # Note that ElementTree elements with children are designed to
            # behave quite a lot like ordinary Python lists, and that's how we
            # regard this one.
            block_elements = source.find("blocks")
            if block_elements:
                if block_elements[0].tag == "block_raw_fidsum":
                    # Prior to Vespa 0.7.0, fidsum was expressed in a special
                    # raw_fidsum block rather than in prep. Here we split the
                    # raw_fidsum element into a regular raw element and a
                    # prep_fidsum element.
                    raw = block_elements[0]

                    # A BlockPrepFidsum looks enough like a BlockRawFidsum
                    # object that we can inflate the former from the latter's
                    # data simply by renaming the settings object.
                    raw_settings = raw.find("block_raw_fidsum_settings")
                    raw_settings.tag = "settings"
                    prep = block_prep_fidsum.BlockPrepFidsum(raw)
                    # Deflate the object back into an ElementTree element and
                    # slip it into the appropriate place amongst it siblings.
                    # This may seem a bit stupid since we just inflated it, but
                    # the goal here is to make the old XML (or the ETree
                    # representation of it) look as much like the new style
                    # as possible.
                    prep = prep.deflate()
                    block_elements.insert(1, prep)

                    # Now make the block_raw_fidsum element look like a plain
                    # old block_raw element.
                    raw.tag = "block_raw"
                    raw.remove(raw_settings)
                    # Replace the data subelement with the raw subelement.
                    raw.remove(raw.find("data"))
                    raw.find("raw").tag = "data"


                if xml_version == "1.0.0":
                    # Version 1.0.0 has a basic block. As of Vespa >= 0.6.3
                    # a.k.a. XML_VERSION 1.1.0, there's no more basic block
                    # but instead a UserPrior object hanging off of the dataset.
                    # Find and extract the basic block and turn it into
                    # a UserPrior object.
                    for i, block_element in enumerate(block_elements):
                        if block_element.tag == "block_basic":
                            break
                    block_basic = block_elements[i]
                    block_elements.remove(block_basic)

                    block_basic_settings = block_basic.find("block_basic_settings")

                    auto_prior = block_basic_settings.find("auto_prior")
                    auto_prior.tag = "user_prior_spectrum"

                    self.user_prior.inflate(block_basic_settings)

                    # Some confusion here...due to an oversight, there are two
                    # versions of XML format 1.0.0. One version has HLVSD as
                    # a separate block/tab, the other has it merged with
                    # spectral. The code below handles the case where HLSVD
                    # is separate.
                    # The SVD block/tab was subsumed into spectral starting
                    # with Vespa 0.6.1.
                    hlsvd = None
                    spectral = None
                    for i, block_element in enumerate(block_elements):
                        if block_element.tag == "block_hlsvd":
                            hlsvd = block_element
                            hlsvd_index = i
                        if block_element.tag == "block_spectral":
                            spectral = block_element

                        if hlsvd and spectral:
                            # move hlsvd attribs to spectral block
                            excludes = ("dim", )
                            for item in hlsvd:
                                if item.tag not in excludes:
                                    spectral.append(item)

                            # scrounge the threshold value from the previous
                            # hlsvd water filter settings if available
                            settings = spectral.find("block_spectral_settings")
                            water_filter = settings.find("water_filter")

                            if water_filter is not None:
                                # Ensure this is the HLSVD filter and not some
                                # other kind.
                                if water_filter.attrib['id'] == _HLSVD_FILTER_UUID:
                                    # Copy threshold info to spectral
                                    threshold = water_filter.findtext("threshold")
                                    util_xml.TextSubElement(settings,
                                                            "svd_threshold",
                                                            threshold)
                                    apply_ = water_filter.findtext("apply_as_water_filter")
                                    util_xml.TextSubElement(settings,
                                                            "svd_apply_threshold",
                                                            apply_)
                            #else:
                                # No water filter, nothing to worry about.

                            block_elements.remove(hlsvd)

                #else:
                    # Nothing to do, the XML version is current.
            else:
                # block_elements is None or has no kids. If either of these
                # conditions occurs, we are in undefined territory since every
                # dataset has at least a raw block.
                raise ValueError("<block> element must have children")

            # end parsing of previous dataset versions
            #==================================================================

            for block_element in block_elements:
                self._create_block(block_element.tag, block_element)

            if xml_version == '1.1.0':
                # BUGFIX v 1.1.0 - the ppm result values were being calc/stored/restored using
                #                  dataset.hz2ppm(..., acq=True) which gave wrong results when
                #                  a zerofill > 1 was used. Because it was set both in checkin
                #                  AND check out, the bug balanced out, and fits were OK to 
                #                  data. But printouts to template and HTML were wrong. Version
                #                  1.1.0 fixed this with conversion as part of inflate()
                pass


        elif hasattr(source, "keys"):
            # Quacks like a dict
            raise NotImplementedError



    #################################################################
    ##### Public functions for Phase0/1 Frequency Shift
    #####
    ##### Phase 0 and 1 are used in a number of processing tabs (for
    ##### View typically). To facilitate set/get of these values,
    ##### which are traditionally stored in the Spectral processing
    ##### module, we create these helper functions at the level of
    ##### the Dataset
    #################################################################

    def get_phase_0(self, xyz):
        """ Returns 0th order phase for the voxel at the xyz tuple """
        return self.blocks["spectral"].get_phase_0(xyz)

    def set_phase_0(self, phase_0, xyz):
        """ Sets 0th order phase for the voxel at the xyz tuple """
        self.blocks["spectral"].set_phase_0(phase_0, xyz)

    def get_phase_1(self, xyz):
        """ Returns 1st order phase for the voxel at the xyz tuple """
        return self.blocks["spectral"].get_phase_1(xyz)

    def set_phase_1(self, phase_1, xyz):
        """ Sets 1st order phase for the voxel at the xyz tuple """
        self.blocks["spectral"].set_phase_1(phase_1, xyz)

    def get_frequency_shift(self, xyz):
        """ Returns frequency_shift for the voxel at the xyz tuple """
        return self.blocks["spectral"].get_frequency_shift(xyz)

    def set_frequency_shift(self, frequency_shift, xyz):
        """ Sets frequency_shift for the voxel at the xyz tuple """
        self.blocks["spectral"].set_frequency_shift(frequency_shift, xyz)

    def get_source_data(self, block_name):
        """
        Returns the data from the first block to the left of the named block
        that is not None

        """
        keys = list(self.blocks.keys())
        keys = keys[0:keys.index(block_name)]
        for key in keys[::-1]:
            data = self.blocks[key].data
            if data is not None:
                return data
        return self.blocks[block_name].data


    def get_source_chain(self, block_name):
        """
        Returns the chain object from the first block to the left of the named
        block that is not None

        """
        keys = list(self.blocks.keys())
        keys = keys[0:keys.index(block_name)]
        for key in keys[::-1]:
            data = self.blocks[key].chain
            if data is not None:
                return data
        return self.blocks[block_name].chain


########################   "Private" Methods    ########################


    def _create_block(self, type_info, attributes=None):
        """
        Given block type info (see below) and optional attributes (suitable
        for passing to inflate()), creates a block of the specified type,
        places it in this dataset's dict of blocks, and returns the
        newly-created block.

        The type_info can be either one of the keys in _XML_TAG_TO_SLOT_CLASS_MAP
        or a 2-tuple of (slot name, class). In the former case,
        _XML_TAG_TO_SLOT_CLASS_MAP is used to look up the 2-tuple. That makes
        this method very convenient for calling from Dataset.inflate().

        In both cases, this method replaces the class in the slot name with
        an instance of the class in the 2-tuple.

        The newly-created block is also asked to create its chain and
        set its dims.
        """
        if type_info in _XML_TAG_TO_SLOT_CLASS_MAP:
            name, klass = _XML_TAG_TO_SLOT_CLASS_MAP[type_info]
        else:
            # If this isn't a tuple, there's going to be trouble!
            name, klass = type_info

        # Instantiate and replace the existing block with the new one.
        block = klass(attributes)
        self.blocks[name] = block
        block.set_dims(self)
        # setting of helper attributes self.raw_xxx depends on self.data
        # having correct dimensions, so this has to follow set_dims()
        block.create_chain(self)

        return block



    def automatic_phasing_max_real_freq(self, freq):
        """ Return phase 0 that produces largest summed area under real data """
        max_freq = -1e40
        for i in range(0,360):
            phase = np.exp(1j * i * common_constants.DEGREES_TO_RADIANS)
            max_  = np.sum((freq * phase).real)
            if max_ > max_freq:
                max_freq = max_
                max_index = i
        return max_index



